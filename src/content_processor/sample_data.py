


llm_dumped_dict_result = {'content_description': "The table presents accuracy results (in percentages) for different methods evaluated on the DocBench Dataset, across two axes: Domains and Types, with an Overall score column. The 'Method' column lists four models (GPT-4o-mini, LightRAG, MMGraphRAG, RAGAnything). 'Domains' are subdivided into Academia (Aca.), Finance (Fin.), Government (Gov.), Law, and News. 'Types' include Text-only (Txt.), Multimodal (Mm.), and Unanswerable queries (Una.). Each cell under Domains and Types shows the accuracy for that method on the respective category. For every category, the numbers are highlighted: the highest accuracy in dark blue, and the second highest in light blue, signifying relative performance leadership. For example, RAGAnything achieves the highest accuracy in Finance (67.0%), while MMGraphRAG leads in Academia (64.3%) and Government (64.9%). In Types, LightRAG and RAGAnything both score highest in Text-only (85.0%), while RAGAnything also leads in Multimodal (76.3%). The 'Overall' column captures the aggregate accuracy for each method, with RAGAnything achieving the best overall (63.4%) and MMGraphRAG as second best (61.0%).",
                           'entity_summary': [{'entity_name': 'DocBench Method Accuracy Table', 'entity_type': 'table', 'related_entities': 'Methods (GPT-4o-mini, LightRAG, MMGraphRAG, RAGAnything), Domains (Academia, Finance, Government, Law, News), Types (Text-only, Multimodal, Unanswerable), Overall Score', 'entity_summary': 'This table lists accuracy scores for four retrieval-augmented generation (RAG) methods, broken down by document domain, question type, and overall performance. The entities—methods, domains, and types—are interconnected: for each method, its specific accuracy is reported in each domain and type, with color highlights marking the top-performing approaches per category. Overall scores provide a comparative benchmark across methods.'}]}

pydantic_dumped_dict_values = ("The table presents the accuracy percentages of different methods across various document domains (Academia, Finance, Government, Legal, News) and query types (Text-only, Multimodal, Unanswerable), as well as overall accuracy on the DocBench dataset. The methods compared are GPT-4o-mini, LightRAG, MMGraphRAG, and RAGAnything. For each method, accuracy scores are reported under each domain and type. The highest accuracy in each column is highlighted in dark blue, and the second-highest in light blue. For example, in the 'Finance' domain, RAGAnything achieves the highest accuracy (67.0%), followed by MMGraphRAG (52.8%). Across most domains and types, either RAGAnything or MMGraphRAG show the top performance, with RAGAnything also achieving the highest overall accuracy (63.4%). LightRAG excels in the 'Txt.' type (85.0%), which is matched by RAGAnything. The unanswerable (Una.) type shows MMGraphRAG leading (60.5%). The numerical values enable direct comparison of performance strengths across both domains and task types for each method.", [{'entity_name': 'DocBench Methods Accuracy Table', 'entity_type': 'table', 'related_entities': 'Methods (GPT-4o-mini, LightRAG, MMGraphRAG, RAGAnything); Domains (Academia, Finance, Government, Law, News); Types (Text-only, Multimodal, Unanswerable); Overall Accuracy', 'entity_summary': 'This table records and compares the accuracy of four retrieval-augmented generation methods on the DocBench dataset, broken down by domain category, query type, and overall score. It systematically relates the performance of each method (rows) to domain and type (columns), highlighting top accuracy values to facilitate identification of strengths within the evaluated settings.'}])



raw_output_of_llm =    """
    {
    "content_description": "The table presents accuracy percentages for different retrieval-augmented generation (RAG) methods evaluated on the DocBench dataset, organized by both domain categories (Academia, Finance, Government, Law, News) and document types (Text-only, Multimodal, Unanswerable queries), alongside an Overall score. Each method’s performance is listed in rows: GPT-4o-mini, LightRAG, MMGraphRAG, and RAGAnything. Notable values are highlighted, where each cell represents the accuracy (%) for a specific method in a particular domain or type. For example, LightRAG shows accuracy values ranging from 46.8 to 85.0 across the cells, with the highest text-only (Txt.) accuracy at 85.0. MMGraphRAG has the highest scores in Academic (64.3), Government (64.9), and Unanswerable (60.5) queries among all methods. RAGAnything records the highest scores in Finance (67.0), News (66.3), Multimodal documents (76.3), and the best Overall accuracy (63.4), with Text-only peaking at 85.0, matching LightRAG. The table does not provide additional statistical analysis but enables direct comparison of methods for each data slice.",
    "entity_summary": [
        {
        "entity_name": "DocBench RAG Methods Accuracy Table",
        "entity_type": "table",
        "related_entities": "Methods (GPT-4o-mini, LightRAG, MMGraphRAG, RAGAnything), Domains (Academia, Finance, Government, Law, News), Types (Text-only, Multimodal, Unanswerable), Accuracy scores (per method, per domain/type, Overall)",      
        "entity_summary": "This table displays the accuracy of various RAG-based methods on the DocBench dataset, segmented by document domains and types, with individual and overall scores. It allows comparison of how each method (row entity) performs across a range of real-world content categories and question types (column entities), enabling assessment of retrieval and reasoning effectiveness of each system."
        }
    ]
    }
    """